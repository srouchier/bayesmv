[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"website promotes use Bayesian data analysis building energy use monitoring, especially context \nMeasurement Verification (M&V) methods.work progress.target audience book energy experts may moderate background statistics applied mathematics. One main arguments promoting Bayesian data analysis universally applicable, accessible. aim give building energy practicioners tools accurately assess savings following energy conservation measures, along uncertainty.first part website introduces reader main concepts building energy monitoring, measurement verification, Bayesian data analysis. amount theoretical background voluntarily kept short order keep focus practical examples. References included readers wish learn .second part tutorials whole-building energy monitoring M&V, referred “Option C” IPMVP documentation.first tutorial serves introduction Stan platform, shows Bayesian analysis obtains uncertainty assessments classical methods, analytical solutions available.reader reassured accuracy method, second tutorial shows effortlessly extends complex model structures, analytical solutions may longer evaluate uncertainty analytically.third tutorial autocorrelation?Option D","code":""},{"path":"index.html","id":"the-content-and-how-to-use-it","chapter":"Welcome","heading":"The content and how to use it","text":"content displayed R notebooks allow displaying full code calculations, results explanations. order apply methods use cases, first:Install R RstudioInstall Stan. can learn Stan basics link full Stan documentation.Additionally, wish replicate results shown tutorials, may either download full book repository run R markdown files matching chapters; download data sets copy/paste code R files.use R main programming language analyses, content also adaptable environments reader may familiar : Python, Julia Matlab.","code":""},{"path":"index.html","id":"about","chapter":"Welcome","heading":"About","text":"contributed website. made.","code":""},{"path":"index.html","id":"license","chapter":"Welcome","heading":"License","text":"good license.","code":""},{"path":"back-mv.html","id":"back-mv","chapter":"1 Measurement and verification","heading":"1 Measurement and verification","text":"","code":""},{"path":"back-mv.html","id":"intro-to-mv","chapter":"1 Measurement and verification","heading":"1.1 Intro to M&V","text":"Minimal background basics M&V. Just essentials website self-sufficient. reading, link resources.","code":""},{"path":"back-mv.html","id":"ipmvp","chapter":"1 Measurement and verification","heading":"1.2 IPMVP","text":"Main concepts IPMVP: 4 options workflows. , keeping short.","code":""},{"path":"back-mv.html","id":"back-stats","chapter":"1 Measurement and verification","heading":"1.3 Statistical concepts","text":"Quick list statistical concepts: uncertainty, bias, correlation, etc.","code":""},{"path":"back-bayes.html","id":"back-bayes","chapter":"2 Bayesian data analysis","heading":"2 Bayesian data analysis","text":"page based presctiptive part contribution UAG guide. need elaborate simplify parts.starting hereBayesian statistics mentioned Annex B ASHRAE Guideline 14, observed standard approaches make difficult estimate savings uncertainty complex models required:“Savings uncertainty can determined exactly energy use linear function independent variable(s). complicated models energy use, changepoint models, data serially autocorrelated errors, approximate formulas must used. approximations provide reasonable accuracy compared simulated data, general difficult determine accuracy given situation. One alternative method determining savings uncertainty desired degree accuracy use Bayesian approach.” references propose apply Bayesian analysis M&V:Shonder, J. ., & Im, P. (2012). Bayesian analysis savings retrofit projects. ASHRAE Transactions, 118, 367.Lira, . (2016). GUM revision: Bayesian view toward expression measurement uncertainty. European Journal Physics, 37(2), 025803.Carstens, H., Xia, X., & Yadavalli, S. (2018). Bayesian energy measurement verification analysis. Energies, 11(2), 380.Several advantages drawbacks mentioned (Carstens 2018), including:Bayesian models probabilistic, uncertainty automatically exactly quantified. Confidence intervals can interpreted way people understand : degrees belief value parameterThe Bayesian approach widely used rapidly gaining popularity scientific fields. (Lira 2016) relates even GUM (adopted many societies physics, chemistry, electrotechnics, etc.) rewritten consistent approach. Since M&V reports uncertainty according GUM, Bayesian calculations useful.Bayesian models universal flexible standard methods. Models also modular can designed suit problem. example, different create terms serial correlation, heteroscedasticity (non-constant variance) specify ordinary linear model.Bayesian models can account model-selection uncertainty.Bayesian approaches allow real-time online updating estimates.Bayesian approach allows incorporation prior information appropriate.savings need calculated “normalised conditions”, example, ‘typical meteorological year’, rather conditions post-retrofit monitoring period, possible quantify uncertainty using current methods. However, Shonder Im (2012) shown can naturally easily quantified using Bayesian approach.points 1 3 perhaps relevant M&V practitioner: can define arbitrary model structure explain data, use exact set formulas obtain savings uncertainty models fitted.","code":""},{"path":"back-bayes.html","id":"step-1-setting-up-a-model","chapter":"2 Bayesian data analysis","heading":"2.0.1 Step 1: setting up a model","text":"Bayesian model made two components:observational model \\(p(y|\\theta)\\) describes relationship data \\(y\\) model parameters \\(\\theta\\)prior model \\(p(\\theta)\\) describes may already know parameters \\(\\theta\\)example, can define linear regression model follows. model output energy use \\(e\\), measured number points baseline period. expect \\(e\\) follow normal distribution centered around linear function outdoor temperature \\(T\\) squared value \\(T^2\\):\\[ e \\sim \\mathcal{N}(\\beta_0 + \\beta_1 T + \\beta_2 T^2, \\sigma)\\]\napproach however much flexible ordinary linear regression. instance, can also assume model uncertainty \\(\\sigma\\) may depend temperature:\\[ \\sigma = \\sigma_0 + \\sigma_1 T\\]\naccounts estimating additional parameters describe eventual dependency prediction residuals temperature.Last, can specify prior distribution parameters. following line imposes search interval parameter \\(\\beta_0\\) uniform distribution\\[ \\beta_0 \\sim \\mathcal{U}(500, 1500)\\]\ntutorial , illustrate Bayesian inference complex change-point model 11 parameters.","code":""},{"path":"back-bayes.html","id":"step-2-learning","chapter":"2 Bayesian data analysis","heading":"2.0.2 Step 2: learning","text":"Bayes’ rule formulates posterior distribution, .e. probability distribution parameters conditioned observed data.\\[ p(\\theta|y) \\propto p(y|\\theta) p(\\theta)\\]practice, posterior calculated exactly, approached large set samples \\(\\theta^{(m)}\\) produced Markov Chain Monte Carlo (MCMC) algorithm. step mostly automated doesn’t require much intervention user, except diagnose solve eventual convergence problems.\\[\\theta^{(m)} \\sim p(\\theta|y)\\]","code":""},{"path":"back-bayes.html","id":"step-3-prediction-and-inference","chapter":"2 Bayesian data analysis","heading":"2.0.3 Step 3: prediction and inference","text":"quantities wish estimate (energy use, savings…) uncertainty directly calculated posterior distribution.-called posterior predictive distribution new observations \\(\\tilde{y}\\) (probability distribution energy use time step reporting period), conditioned training data set \\(y\\), can calculated calling observational model large enough number \\(\\theta^{(m)}\\) draws:\\[ p\\left(\\tilde{y}|y\\right) \\approx \\frac{1}{M} \\sum_{m=1}^M p\\left(\\tilde{y}|\\theta^{(m)}\\right) \\]\nadvantage methodology steps 2 3 always remain , matter model structure chosen step 1. practitioner may try different formulations priors, essentially re-run script see model checks validation requirements, automatically obtaining savings uncertainty.","code":""},{"path":"back-bayes.html","id":"stan","chapter":"2 Bayesian data analysis","heading":"2.1 Stan","text":"tools use Bayesian data analysisPython, R, Julia MatlabStan, pymc, pyro otherWe use Stan, interfaced R, content website adaptable tools.","code":""},{"path":"optCbasics.html","id":"optCbasics","chapter":"IPMVP Option C","heading":"IPMVP Option C","text":"Introduction option C essentials. use cases workflows","code":""},{"path":"optClinreg.html","id":"optClinreg","chapter":"3 Tutorial 1: linear regression and introduction to Stan","heading":"3 Tutorial 1: linear regression and introduction to Stan","text":"first tutorial uses simple model, ordinary linear regression, show Bayesian method returns results (energy use predictions, savings, uncertainty) analytical method. bonus, also tutorial Stan.introduction Stancomparison classical analysisImportsThe data","code":"\nlibrary(rstan)\nlibrary(tidyverse)\n# setwd(\"...\") #Set this to path of \"cddhddmod.stan\"\n\n# Baseline data: cooling and heating degree days, energy use\ndf.pre <- data.frame(cdd = c(0, 10, 15, 21, 21, 14, 0, 0, 0, 0, 0, 0),\n                     hdd = c(14, 0, 0, 0, 0, 0, 2, 20, 18, 32, 27, 20),\n                     use = c(35936, 22260, 20970, 25438, 28547, 24394,\n                             24224, 38767, 42205, 49649, 43540, 43734))\n# Reporting period data: cooling and heating degree days, energy use\ndf.post <- data.frame(cdd = c(0, 3, 13, 23, 19, 16, 0, 0, 0, 0, 0, 0),\n                      hdd = c(5, 0, 0, 0, 0, 0, 2, 19, 20, 24, 25, 14),\n                      use = c(25416, 17445, 13626, 19295, 17970, 15158,\n                              13293, 35297, 35665, 38562, 33383, 37244))"},{"path":"optClinreg.html","id":"modelling","chapter":"3 Tutorial 1: linear regression and introduction to Stan","heading":"3.1 Modelling","text":"equationstan modelexplanation blocs:dataparametersmodel y ~ normal(x * beta + alpha, sigma);generated quantitiesThe last block generated quantities optional.","code":"\nmodel <- \"\ndata {\n// Baseline data\n  int<lower=0> N;     // number of data items\n  int<lower=0> K;     // number of predictors\n  matrix[N, K] x;     // predictor matrix\n  vector[N] y;        // outcome vector\n// Reporting period data\n  int<lower=0> N_post;  // number of data items\n  matrix[N, K] x_post;     // predictor matrix\n  vector[N] y_post;        // outcome vector\n}\nparameters {\n  real alpha;           // intercept\n  vector[K] beta;       // coefficients for predictors\n  real<lower=0> sigma;  // error scale\n }\nmodel {\n  y ~ normal(x * beta + alpha, sigma);  // likelihood\n}\ngenerated quantities {\n  vector[N_post] prediction;\n  real savings = 0;\n  for (n in 1:N_post) {\n    prediction[n] = normal_rng(x_post[n] * beta + alpha, sigma);\n    savings += prediction[n] - y_post[n];\n  }\n}\n\""},{"path":"optClinreg.html","id":"model-training","chapter":"3 Tutorial 1: linear regression and introduction to Stan","heading":"3.1.1 Model training","text":"link options stan() method. works now ’ll use options later tutorial\ndefault : 4 chains, 2000 samples per chain, 1000 warmup. case convergence issues, increasing samples warmup one first things try.","code":"\ndata_list <- list(N = nrow(df.pre),\n                  K = 2,\n                  x = df.pre %>% select(cdd, hdd),\n                  y = df.pre$use,\n                  N_post = nrow(df.post),\n                  x_post = df.post %>% select(cdd, hdd),\n                  y_post = df.post$use)\n\nfit1 <- stan(model_code = model, data = data_list, refresh = 0)\n\nprint(fit1, pars=c(\"alpha\", \"beta\", \"sigma\", \"savings\"), probs=c(.025,.5,.975))## Inference for Stan model: 518e88093c164eed938c981d4795b52f.\n## 4 chains, each with iter=2000; warmup=1000; thin=1; \n## post-warmup draws per chain=1000, total post-warmup draws=4000.\n## \n##             mean se_mean       sd     2.5%      50%     97.5% n_eff Rhat\n## alpha   22816.62   74.70  2530.15 17813.11 22838.03  27791.20  1147    1\n## beta[1]   117.15    4.87   168.98  -217.76   116.54    446.14  1204    1\n## beta[2]   874.67    3.59   124.37   633.33   872.67   1131.50  1198    1\n## sigma    3096.13   24.75   871.05  1923.38  2943.78   5216.65  1239    1\n## savings 75604.20  277.61 16176.65 44126.99 75402.61 108854.37  3396    1\n## \n## Samples were drawn using NUTS(diag_e) at Tue Feb 21 11:00:06 2023.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1)."},{"path":"optClinreg.html","id":"convergence-diagnostics","chapter":"3 Tutorial 1: linear regression and introduction to Stan","heading":"3.1.2 Convergence diagnostics","text":"","code":""},{"path":"optClinreg.html","id":"validation","chapter":"3 Tutorial 1: linear regression and introduction to Stan","heading":"3.2 Validation","text":"Validation steps classical analysis","code":"\nla <- rstan::extract(fit1, permuted = TRUE)\nresiduals <- colMeans(la$prediction) - df.pre$use\nacf(residuals)"},{"path":"optClinreg.html","id":"prediction-of-savings","chapter":"3 Tutorial 1: linear regression and introduction to Stan","heading":"3.3 Prediction of savings","text":"Comparison classical analysispoint pointtotal savingsdisplay results table?Conclusion : get results approaches. Bayesian method seems much complicated classical method ’ll show next tutorials remain regardless model complexity, classical approach keep ","code":"\nfit2 <- lm(use ~ cdd + hdd, df.pre)\nsummary(fit2)## \n## Call:\n## lm(formula = use ~ cdd + hdd, data = df.pre)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3608.1 -1596.3  -225.6  1464.2  3636.7 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  22860.4     2136.0  10.702 2.03e-06 ***\n## cdd            114.5      142.6   0.803    0.443    \n## hdd            872.7      104.2   8.374 1.53e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2681 on 9 degrees of freedom\n## Multiple R-squared:  0.9421, Adjusted R-squared:  0.9292 \n## F-statistic: 73.24 on 2 and 9 DF,  p-value: 2.702e-06\npredict.1 <- apply(la$prediction, 2, quantile, probs=c(0.025, 0.5, 0.975))\n\npredict.2 <- predict(fit2, newdata = df.post, interval = \"prediction\", level=0.95)\n\nxplot <- array(1:nrow(df.post))\nggplot(mapping = aes(x = xplot)) +\n  geom_point(mapping = aes(y = df.post$use)) +\n  geom_line(mapping = aes(y = predict.1[2,]), color='red') +\n  geom_line(mapping = aes(y = predict.2[,1]), color='blue') +\n  geom_ribbon(mapping = aes(ymin=predict.1[1,], ymax=predict.1[3,]), fill='red', alpha=0.1) +\n  geom_ribbon(mapping = aes(ymin=predict.2[,2], ymax=predict.2[,3]), fill='blue', alpha=0.1)\nmethod <- c(\"bayesian\", \"classical\")\n# Mean savings\nmean.savings <- c(mean(la$savings),\n                  sum(predict.2[,1] - df.post$use))\n# Lower and upper bounds of the 95% confidence interval of savings\nlower.bound <- c(sum(predict.1[1,] - df.post$use),\n                 sum(predict.2[,2] - df.post$use))\nupper.bound <- c(sum(predict.1[3,] - df.post$use),\n                 sum(predict.2[,3] - df.post$use)) \nsvgs <- data.frame(method, mean.savings, lower.bound, upper.bound)\n\nprint(svgs)##      method mean.savings lower.bound upper.bound\n## 1  bayesian     75604.20  -11293.806    161877.8\n## 2 classical     75564.54   -6385.537    157514.6"},{"path":"optCchangepoint.html","id":"optCchangepoint","chapter":"4 Tutorial 2: change-point model","heading":"4 Tutorial 2: change-point model","text":"based option C example already wrote. need simplify parts , elaborate parts.data used example hourly energy consumption outdoor air temperature data 11 commercial buildings (office/retail), publicly available :https://openei.org/datasets/dataset/consumption-outdoor-air-temperature-11-commercial-buildingsWe using two data files, respectively labeled Building 6 (Office “Pre”), Building 6 (Office “Post”).","code":"\nlibrary(rstan)\nlibrary(tidyverse)\nlibrary(lubridate)"},{"path":"optCchangepoint.html","id":"the-data","chapter":"4 Tutorial 2: change-point model","heading":"4.1 The data","text":"","code":""},{"path":"optCchangepoint.html","id":"loading-and-displaying","chapter":"4 Tutorial 2: change-point model","heading":"4.1.1 Loading and displaying","text":"following block loads two separate data files:building60preoffice.csv baseline period file, saved df.base variablebuilding60postoffice.csv reporting period file, saved df.repo variableThe Date column files converted DateTime type new column. , baseline dataset displayed first exploratory look data.data hourly time step size. Every hour, outdoor air temperature (OAT °F) energy use (kW) available.energy use higher summer winter -. suggests consumption data includes heating cooling appliances.Week-ends clearly visible lower consumption working days week.","code":"\n# Baseline data: one year\ndf.base <- read_csv(\"data/Long-term 11 commercial buildings/building60preoffice.csv\") %>% \n  mutate(DateTime = mdy_hm(Date),\n         Date = as_date(DateTime))\n\n# Post-retrofit data: one year\ndf.repo <- read_csv(\"data/Long-term 11 commercial buildings/building61duringoffice.csv\") %>% \n  mutate(DateTime = mdy_hm(Date),\n         Date = as_date(DateTime))\n\n# Plot the original data\nhead(df.base)## # A tibble: 6 x 4\n##   Date         OAT `Building 6 kW` DateTime           \n##   <date>     <dbl>           <dbl> <dttm>             \n## 1 2009-01-02  41.6            23.3 2009-01-02 00:00:00\n## 2 2009-01-02  40.9            23.1 2009-01-02 01:00:00\n## 3 2009-01-02  39.5            23.7 2009-01-02 02:00:00\n## 4 2009-01-02  36.3            29.1 2009-01-02 03:00:00\n## 5 2009-01-02  32.8            35.6 2009-01-02 04:00:00\n## 6 2009-01-02  32.5            45.5 2009-01-02 05:00:00\nggplot(data = df.base) + geom_line(mapping = aes(x=DateTime, y=`Building 6 kW`))"},{"path":"optCchangepoint.html","id":"subset-selection","chapter":"4 Tutorial 2: change-point model","heading":"4.1.2 Subset selection","text":"Averaging data daily time steps allow overlook dependence consecutive measurements. turn, allows using model much simpler time series models, capable low frequency predictions.following block creates new datasets original ones:Measurements daily averagedTemperatures switched °C international suitability.categorical variable added indicate week ends., plot daily energy use \\(E\\) (kWh) versus outdoor temperature \\(T\\) (°C) values week.end categorical variable.seems fitting train separate models working days week ends, energy use clearly different two sets. tutorial, consider working days, aren’t many available data points week ends, possible .","code":"\ndaily.average <- function(df) {\n  df %>% \n    group_by(Date) %>% \n    summarise(OAT = mean(OAT),\n              E = sum(`Building 6 kW`),\n              .groups = 'drop'\n    ) %>% \n    mutate(wday = wday(Date),\n           week.end = wday==1 | wday==7,\n           T = (OAT-32) * 5/9)\n}\n\ndf.base.daily <- daily.average(df.base)\ndf.repo.daily <- daily.average(df.repo)\n\nggplot(data = df.base.daily) +\n  geom_point(mapping = aes(x=T, y=E, color=week.end))\ndf.base.daily <- daily.average(df.base) %>% filter(!week.end)\ndf.repo.daily <- daily.average(df.repo) %>% filter(!week.end)"},{"path":"optCchangepoint.html","id":"modelling-and-training","chapter":"4 Tutorial 2: change-point model","heading":"4.2 Modelling and training","text":"","code":""},{"path":"optCchangepoint.html","id":"model-definition","chapter":"4 Tutorial 2: change-point model","heading":"4.2.1 Model definition","text":"looking data, can suggest using change-point model include effects heating cooling, separate week ends working days. expected daily energy use \\(E\\) (kWh per day) function outdoor temperature \\(T\\) number parameters:\\[p(E|T) \\sim \\mathcal{N}\\left[\\alpha + \\beta_{h}(\\tau_{h}-T)^+ + \\beta_{c}(T-\\tau_{c})^+, \\sigma\\right]\\]\\(h\\) \\(c\\) subscripts indicate heating cooling modes. \\(+\\) superscript indicates term applied zero.equation mean expect energy use \\(E\\) normal distribution centered around change-point model, constant standard deviation \\(\\sigma\\). particularities Bayesian statistics : normal distribution can replaced probability distribution; error term \\(\\sigma\\) can formulated function inputs; etc.model 6 possible parameters, ordinary linear regression. following method also exactly decided complexify model, instance assuming non-linear profiles side change points, categorical variables.","code":""},{"path":"optCchangepoint.html","id":"model-specification-with-stan","chapter":"4 Tutorial 2: change-point model","heading":"4.2.2 Model specification with Stan","text":"example, use Stan probabilistic programming language, allows full Bayesian statistical inference.https://mc-stan.org/STAN model block text can either written separate file, script current code. Specifying model STAN takes certain learning curve, unlocks full flexibility Bayesian analysis., list called model_data created, maps part data appropriate variable STAN model.","code":"\nchangepoint <- [1679 chars quoted with '\"']\nmodel_data <- list(\n  N_base = nrow(df.base.daily),\n  t_base = df.base.daily$T,\n  y_base = df.base.daily$E,\n  N_repo = nrow(df.repo.daily),\n  t_repo = df.repo.daily$T,\n  y_repo = df.repo.daily$E\n)"},{"path":"optCchangepoint.html","id":"step-2-model-fitting","chapter":"4 Tutorial 2: change-point model","heading":"4.2.3 Step 2: Model fitting","text":"Now model specified data mapped variables, syntax model fitting .One disadvantage Bayesian inference MCMC algorithm takes much longer converge typical least-squares model fitting method. Running code might take minute using 365 data points, Bayesian approach might become problematic larger data files (100,000 rows ).Fitting may result number warnings, telling us problems may occurred: divergent transitions, large R-hat values, low Effective Sample Size… Obtaining fit without warnings takes practice essential unbiased interpretation inferred variables predictions. guide Stan’s warnings address available : https://mc-stan.org/misc/warnings.htmlThe first step solving warnings re-run algorithm different controls: iter, max_treedepth, etc. problems persist, possible model complex information data able provide simplified, stronger priors proposed. lot problems can solved prior information. specific case, especially useful variables equation week-ends, since lot data points.","code":"\n# Fitting the model\nfit1 <- stan(\n  model_code = changepoint,  # Stan program\n  data = model_data,        # named list of data\n  chains = 4,               # number of Markov chains\n  warmup = 1000,            # number of warmup iterations per chain\n  iter = 2000,              # total number of iterations per chain\n  cores = 2,                # number of cores (could use one per chain)\n  refresh = 0,              # progress not shown\n)"},{"path":"optCchangepoint.html","id":"validation-and-results","chapter":"4 Tutorial 2: change-point model","heading":"4.3 Validation and results","text":"Stan returns object (called fit1 ) distributions outputs parameters fitted model can accessedhttps://cran.r-project.org/web/packages/rstan/vignettes/stanfit-objects.htmlThe MCMC algorithm produces chain samples \\(\\theta^{(m)}\\) parameters, approximate posterior distributions. case, parameter model represented chain 6,000 draws: draws, can extract statistics need: mean, median, quantiles, \\(t\\)-score \\(p\\)-values, etc.","code":""},{"path":"optCchangepoint.html","id":"parameters","chapter":"4 Tutorial 2: change-point model","heading":"4.3.1 Parameters","text":"first validation step, useful take look values parameters estimated algorithm. , use three diagnostics tools:print method shows table parameters, much like display ordinary linear regressiontraceplot shows traces selected parameters. fitting converged, traces approximate posterior distributionspairs shows pairwise relationships parameters. Strong interactions parameters indication model re-parameterised.","code":"\nprint(fit1, pars = c(\"alpha\", \"beta_h\", \"tau_h\", \"beta_c\", \"tau_c\", \"sigma\", \"savings\"))## Inference for Stan model: 3824f231226c57790491f93819ce4cf5.\n## 4 chains, each with iter=2000; warmup=1000; thin=1; \n## post-warmup draws per chain=1000, total post-warmup draws=4000.\n## \n##             mean se_mean      sd     2.5%      25%      50%      75%    97.5%\n## alpha     829.61    0.25   11.72   805.36   822.07   830.16   837.58   851.50\n## beta_h     33.40    0.05    2.66    28.16    31.63    33.43    35.21    38.50\n## tau_h       6.50    0.02    0.74     5.29     5.98     6.43     6.95     8.19\n## beta_c     29.24    0.06    3.23    23.37    26.91    29.06    31.38    35.87\n## tau_c      15.77    0.02    0.97    13.74    15.12    15.83    16.46    17.51\n## sigma      96.55    0.07    4.11    88.81    93.74    96.42    99.14   105.25\n## savings 46438.39   36.69 2283.24 41912.88 44941.80 46424.15 47939.78 50912.30\n##         n_eff Rhat\n## alpha    2115    1\n## beta_h   2487    1\n## tau_h    1838    1\n## beta_c   2542    1\n## tau_c    2013    1\n## sigma    3919    1\n## savings  3873    1\n## \n## Samples were drawn using NUTS(diag_e) at Tue Feb 28 12:56:27 2023.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\ntraceplot(fit1, pars = c(\"alpha\", \"beta_h\", \"tau_h\", \"beta_c\", \"tau_c\", \"sigma\", \"lp__\"))\npairs(fit1, pars = c(\"alpha\", \"beta_h\", \"tau_h\", \"beta_c\", \"tau_c\", \"sigma\", \"savings\"))"},{"path":"optCchangepoint.html","id":"predictions","chapter":"4 Tutorial 2: change-point model","heading":"4.3.2 Predictions","text":"https://mc-stan.org/docs/2_26/stan-users-guide/posterior-prediction-chapter.htmlOur main goal compare energy use measured reporting period \\(y_\\mathit{repo}\\) predictions fitted model. Since probabilistic model, outcome actually probability distribution \\(p\\left(y_\\mathit{repo}|x_\\mathit{repo}, x_\\mathit{base}, y_\\mathit{base}\\right)\\), based observed values model inputs \\(x\\) baseline reporting periods, observed energy use baseline period \\(y_\\mathit{base}\\).-called posterior predictive distribution \\(p\\left(y_\\mathit{repo}|...\\right)\\) already directly available, value \\(y_\\mathit{repo}\\) (time step) directly calculated Stan model value \\(\\theta^{(m)}\\).\\[ p\\left(y_\\mathit{repo}|...\\right) \\approx \\frac{1}{M} \\sum_{m=1}^M p\\left(y_\\mathit{repo}|x_\\mathit{repo},\\theta^{(m)}\\right) \\]\nFirst, let us look posterior predictive distribution baseline period, order validate model compared training data:","code":"\n# Extracting full predictive distributions from the stanfit object\nla <- rstan::extract(fit1, permuted = TRUE)\ny_base_pred <- la$y_base_pred\n\n# Quantiles\ny_base_quan <- apply(y_base_pred, 2, quantile, probs=c(0.025, 0.5, 0.975))\n\n# Adding prediction quantiles to the baseline dataframe\ndf.base.daily <- df.base.daily %>% mutate(pred_low = y_base_quan[1, ],\n                                          pred_med = y_base_quan[2, ],\n                                          pred_up = y_base_quan[3, ])\n\n# Plot\nggplot(data = df.base.daily) +\n  geom_point(mapping = aes(x=T, y=E)) +\n  geom_line(mapping = aes(x=T, y=pred_med), color='red') +\n  geom_ribbon(mapping = aes(x=T, ymin=pred_low, ymax=pred_up), fill='red', alpha=0.1)"},{"path":"optCchangepoint.html","id":"residuals","chapter":"4 Tutorial 2: change-point model","heading":"4.3.3 Residuals","text":"important validation step check autocorrelation residuals fitted model, baseline data used fitting. Autocorrelation often sign insufficient model complexity, form model error term appropriately chosen.two graphs show:Residuals vs Date, order display eventual autocorrelationresiduals vs Temperature","code":"\nggplot(data = df.base.daily) +\n  geom_line(mapping = aes(x=Date, y=pred_med-E)) +\n  geom_ribbon(mapping = aes(x=Date, ymin=pred_low-E, ymax=pred_up-E), alpha=0.2)\nresiduals <- colMeans(la$y_base_pred) - df.base.daily$E\n\nggplot() +\n  geom_point(mapping = aes(x=residuals, y=lag(residuals, n=1, default=0)))\nacf(residuals)"},{"path":"optCchangepoint.html","id":"savings","chapter":"4 Tutorial 2: change-point model","heading":"4.4 Savings","text":"Stan model already calculates expected output \\(y\\) reporting period, sample \\(\\theta_i\\) posterior distribution. can therefore display probability distribution data points reporting period, compare measured data period.following graph compares energy use measured reporting period (points) probability distributions energy use predicted model period.\nsavings, .e. difference measured energy use reporting period prediction model, included Stan model definition. Similarly prediction, savings therefore available probability distribution: full description confidence interval may wish .table results shown model fitting shows thatThe mean estimated savings 69,069 kWhThe 95% confidence interval spans 63,550 74,880 kWhWe can also choose display quantile posterior distribution savings:However savings estimate may biased. autocorrelated residuals suggest ","code":"\n# Extracting full predictive distributions from the stanfit object\ny_repo_pred <- la$y_repo_pred\n# Quantiles\ny_repo_quan <- apply(y_repo_pred, 2, quantile, probs=c(0.025, 0.5, 0.975))\n# Data frame\ndf.repo.daily <- df.repo.daily %>% mutate(pred_low = y_repo_quan[1, ],\n                                          pred_med = y_repo_quan[2, ],\n                                          pred_up = y_repo_quan[3, ])\n# Plot\nggplot(data = df.repo.daily) +\n  geom_point(mapping = aes(x=T, y=E)) +\n  geom_line(mapping = aes(x=T, y=pred_med), color='red') +\n  geom_ribbon(mapping = aes(x=T, ymin=pred_low, ymax=pred_up), fill='red', alpha=0.1)\nplot(fit1, pars = c(\"savings\"))## ci_level: 0.8 (80% intervals)## outer_level: 0.95 (95% intervals)"},{"path":"optCautocorr.html","id":"optCautocorr","chapter":"5 Tutorial 3: autocorrelation","heading":"5 Tutorial 3: autocorrelation","text":"follow-tutorial 2 explain autocorrelation deal .fitted model autocorrelated residuals, two options :possible, improve model better description building physics;possible improve model , data insufficient learn additional parameters, autocorrelation remains, can somehow “include ” model.latter accounts accepting reached ceiling model refinement, making reliable predictions come cost increased uncertainty.Consider general model formulation observed variable (energy use) \\(y\\) time \\(t\\) expected\\[\ny_t = f(T_t, occ_t) + \\varepsilon_t\n\\]\\(f\\) can function observed variables (ambient temperature, occupancy, etc.)moving average (MA) model uses previous errors predictors future outcomes (see Stan documentation). MA model order \\(Q\\), \\(\\mathrm{MA}(Q)\\) add regression coefficients \\(\\theta_q\\) previous error terms.\\[\ny_t = f(T_t, occ_t) + \\theta_1 \\varepsilon_{t-1} + \\dots + \\theta_Q \\varepsilon_{t-Q} + \\varepsilon_t\n\\]see Stan doc moving average models: https://mc-stan.org/docs/stan-users-guide/moving-average-models.htmlThis model much harder time converging. obtain good mixing high warmupARIMA prediction intervals : see https://otexts.com/fpp2/arima-forecasting.html\\[\n\\hat{\\sigma}_h=\\hat{\\sigma}^2 \\left[1+\\sum_{=1}^{h-1}\\hat{\\theta}_i^2\\right]\n\\]","code":"\nlibrary(rstan)\nlibrary(tidyverse)\nlibrary(lubridate)\n# Baseline data: one year\ndf.pre <- read_csv(\"data/Long-term 11 commercial buildings/building60preoffice.csv\") %>% \n  mutate(DateTime = mdy_hm(Date),\n         Date = as_date(DateTime))\n\n# Post-retrofit data: one year\ndf.post <- read_csv(\"data/Long-term 11 commercial buildings/building61duringoffice.csv\") %>% \n  mutate(DateTime = mdy_hm(Date),\n         Date = as_date(DateTime))\n\n# Plot the original data\nhead(df.pre)## # A tibble: 6 x 4\n##   Date         OAT `Building 6 kW` DateTime           \n##   <date>     <dbl>           <dbl> <dttm>             \n## 1 2009-01-02  41.6            23.3 2009-01-02 00:00:00\n## 2 2009-01-02  40.9            23.1 2009-01-02 01:00:00\n## 3 2009-01-02  39.5            23.7 2009-01-02 02:00:00\n## 4 2009-01-02  36.3            29.1 2009-01-02 03:00:00\n## 5 2009-01-02  32.8            35.6 2009-01-02 04:00:00\n## 6 2009-01-02  32.5            45.5 2009-01-02 05:00:00\nggplot(data = df.pre) + geom_line(mapping = aes(x=DateTime, y=`Building 6 kW`))\ndaily.average <- function(df) {\n  df %>% \n    group_by(Date) %>% \n    summarise(OAT = mean(OAT),\n              E = sum(`Building 6 kW`),\n              .groups = 'drop'\n    ) %>% \n    mutate(wday = wday(Date),\n           week.end = wday==1 | wday==7,\n           T = (OAT-32) * 5/9)\n}\n\ndf.pre.daily <- daily.average(df.pre)\ndf.post.daily <- daily.average(df.post)\n\nggplot(data = df.pre.daily) +\n  geom_point(mapping = aes(x=T, y=E, color=week.end))\ndf.pre.daily <- daily.average(df.pre) %>% filter(!week.end)\ndf.post.daily <- daily.average(df.post) %>% filter(!week.end)\nchangepoint <- [2239 chars quoted with '\"']\nmodel_data <- list(\n  N_pre = nrow(df.pre.daily),\n  t_pre = df.pre.daily$T,\n  y_pre = df.pre.daily$E,\n  N_post = nrow(df.post.daily),\n  t_post = df.post.daily$T,\n  y_post = df.post.daily$E\n)\n# Fitting the model\nfit1 <- stan(\n  model_code = changepoint,  # Stan program\n  data = model_data,        # named list of data\n  chains = 4,               # number of Markov chains\n  warmup = 4000,            # number of warmup iterations per chain\n  iter = 6000,              # total number of iterations per chain\n  cores = 2,                # number of cores (could use one per chain)\n  refresh = 1,              # progress not shown\n)\nprint(fit1, pars = c(\"alpha\", \"beta_h\", \"tau_h\", \"beta_c\", \"tau_c\", \"sigma\", \"theta\", \"savings\"))## Inference for Stan model: dc86eba33ef72f3cda80b6310ccd1c15.\n## 4 chains, each with iter=6000; warmup=4000; thin=1; \n## post-warmup draws per chain=2000, total post-warmup draws=8000.\n## \n##             mean se_mean      sd     2.5%      25%      50%      75%    97.5%\n## alpha     829.16    0.19   13.95   801.36   819.84   829.26   838.66   855.81\n## beta_h     32.00    0.03    2.65    27.08    30.11    31.91    33.82    37.27\n## tau_h       6.76    0.01    0.83     5.23     6.12     6.79     7.35     8.35\n## beta_c     26.66    0.04    2.67    22.00    24.81    26.48    28.31    32.42\n## tau_c      15.02    0.01    0.98    13.28    14.35    14.92    15.64    17.12\n## sigma      78.15    0.04    3.24    72.07    75.93    78.10    80.28    84.68\n## theta       0.57    0.00    0.04     0.49     0.55     0.57     0.60     0.65\n## savings 46887.28   27.69 2481.15 41960.57 45229.12 46894.57 48582.13 51714.05\n##         n_eff Rhat\n## alpha    5482    1\n## beta_h   6266    1\n## tau_h    5219    1\n## beta_c   5679    1\n## tau_c    4804    1\n## sigma    7891    1\n## theta    8240    1\n## savings  8026    1\n## \n## Samples were drawn using NUTS(diag_e) at Tue Feb 28 13:59:55 2023.\n## For each parameter, n_eff is a crude measure of effective sample size,\n## and Rhat is the potential scale reduction factor on split chains (at \n## convergence, Rhat=1).\ntraceplot(fit1, pars = c(\"alpha\", \"beta_h\", \"tau_h\", \"beta_c\", \"tau_c\", \"sigma\", \"lp__\"))\npairs(fit1, pars = c(\"alpha\", \"beta_h\", \"tau_h\", \"beta_c\", \"tau_c\", \"sigma\", \"theta\"))\n# Extracting full predictive distributions from the stanfit object\nla <- rstan::extract(fit1, permuted = TRUE)\ny_pre_pred <- la$y_pre_pred\n\n# Quantiles\ny_pre_quan <- apply(y_pre_pred, 2, quantile, probs=c(0.025, 0.5, 0.975))\n\n# Adding prediction quantiles to the baseline dataframe\ndf.pre.daily <- df.pre.daily %>% mutate(pred_low = y_pre_quan[1, ],\n                                          pred_med = y_pre_quan[2, ],\n                                          pred_up = y_pre_quan[3, ])\n\n# Plot\nggplot(data = df.pre.daily) +\n  geom_point(mapping = aes(x=T, y=E)) +\n  geom_line(mapping = aes(x=T, y=pred_med), color='red') +\n  geom_ribbon(mapping = aes(x=T, ymin=pred_low, ymax=pred_up), fill='red', alpha=0.1)\nggplot(data = df.pre.daily) +\n  geom_line(mapping = aes(x=Date, y=pred_med-E)) +\n  geom_ribbon(mapping = aes(x=Date, ymin=pred_low-E, ymax=pred_up-E), alpha=0.2)\nresiduals <- colMeans(la$y_pre_pred) - df.pre.daily$E\n\nggplot() +\n  geom_point(mapping = aes(x=residuals, y=lag(residuals, n=1, default=0)))\nggplot() +\n  geom_point(mapping = aes(x=residuals, y=lag(residuals, n=2, default=0)))\nacf(residuals)"},{"path":"optDbasics.html","id":"optDbasics","chapter":"Option D basics","heading":"Option D basics","text":"Just essentials.","code":""},{"path":"bayesian-calibration.html","id":"bayesian-calibration","chapter":"6 Bayesian calibration","heading":"6 Bayesian calibration","text":"literature principles “bayesian calibration”, Option D can done bayesically.","code":""}]
